{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f40a9aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import xlsxwriter\n",
    "import numpy as np\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f51fe7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " text (InputLayer)              [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " preprocessing (KerasLayer)     {'input_word_ids':   0           ['text[0][0]']                   \n",
      "                                (None, 128),                                                      \n",
      "                                 'input_mask': (Non                                               \n",
      "                                e, 128),                                                          \n",
      "                                 'input_type_ids':                                                \n",
      "                                (None, 128)}                                                      \n",
      "                                                                                                  \n",
      " BERT_encoder (KerasLayer)      {'encoder_outputs':  109482241   ['preprocessing[0][0]',          \n",
      "                                 [(None, 128, 768),               'preprocessing[0][1]',          \n",
      "                                 (None, 128, 768),                'preprocessing[0][2]']          \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768)],                                               \n",
      "                                 'sequence_output':                                               \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 'pooled_output': (                                               \n",
      "                                None, 768),                                                       \n",
      "                                 'default': (None,                                                \n",
      "                                768)}                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,482,241\n",
      "Trainable params: 109,482,240\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model = tf.keras.models.load_model('models\\model-improvement-34-0.98_noDense768_0.05Val')\n",
    "\n",
    "intermediate_layer_model = tf.keras.Model(inputs=loaded_model.get_layer(\"text\").input,\n",
    "                                       outputs=loaded_model.get_layer(\"BERT_encoder\").output)\n",
    "\n",
    "\n",
    "intermediate_layer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcff7845",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\MOHSEN-ASUS\\JupyterProjects\\bert classification\\final_dataset_for_bert.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa701948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Antelope', 'grizzly+bear', 'killer+whale', 'beaver', 'dalmatian',\n",
       "       'persian+cat', 'horse', 'german+shepherd', 'blue+whale',\n",
       "       'siamese+cat', 'skunk', 'mole', 'tiger', 'hippopotamus', 'leopard',\n",
       "       'moose', 'spider+monkey', 'humpback+whale', 'elephant', 'gorilla',\n",
       "       'ox', 'fox', 'sheep', 'seal', 'chimpanzee', 'hamster', 'squirrel',\n",
       "       'rhinoceros', 'rabbit', 'bat', 'giraffe', 'wolf', 'chihuahua',\n",
       "       'rat', 'weasel', 'otter', 'buffalo', 'zebra', 'giant+panda',\n",
       "       'deer', 'bobcat', 'pig', 'lion', 'mouse', 'polar+bear', 'collie',\n",
       "       'Walrus', 'raccoon', 'cow', 'dolphin'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"class\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "362134ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BERTembbedings(text, model):\n",
    "    \n",
    "    text = [text]\n",
    "    text_tensor = tf.convert_to_tensor(text, dtype=tf.string)\n",
    "    model_output = model(text_tensor)\n",
    "    \n",
    "    return model_output[\"pooled_output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23e8a96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embbedings_labels_generator(dataset, selected_classes, BERT, embbeding_size=768):\n",
    "    \n",
    "    # an array for labels\n",
    "    labels = np.array([])\n",
    "    \n",
    "    # in the line below i am creating a zero array to append my embbedings to it (i dont know how to do it without creating a zero)\n",
    "    class_texts_embbedings = np.zeros([1, embbeding_size])\n",
    "    \n",
    "    for i in selected_classes:\n",
    "        \n",
    "        # getting all of the texts belong to class i\n",
    "        class_texts = dataset[dataset[\"class\"] == i]\n",
    "        class_texts = class_texts[\"text\"]\n",
    "        \n",
    "        for j in class_texts:\n",
    "            \n",
    "            # obtaining embbedings of class i texts using \"get_BERTembbedings\" and appending them to an array\n",
    "            class_texts_embbedings = np.append(class_texts_embbedings, get_BERTembbedings(j, BERT), axis=0)\n",
    "            \n",
    "            # putting labels in labels array\n",
    "            labels = np.append(labels, i)  \n",
    "            \n",
    "        \n",
    "    # deleting the zero array from the first row\n",
    "    class_texts_embbedings = np.delete(class_texts_embbedings, 0, 0)\n",
    "    \n",
    "    return class_texts_embbedings, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad79c617",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = [\"Antelope\"]\n",
    "\n",
    "my_embbedings, my_labels = embbedings_labels_generator(data, class_name, intermediate_layer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "218debec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 2, 0, 2,\n",
       "       2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2,\n",
       "       2, 2, 2, 1, 2, 2, 0, 0, 2, 2, 0, 2, 2, 1, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km = KMeans(n_clusters=3)\n",
    "cluster_prediction = km.fit_predict(my_embbedings)\n",
    "cluster_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5380ee9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def att_generator(embbedings, num_clusters=3):\n",
    "    \n",
    "    #clustering embbedings\n",
    "    km = KMeans(n_clusters=num_clusters)\n",
    "    cluster_prediction = km.fit_predict(my_embbedings)\n",
    "    \n",
    "    #finding the dominant cluster\n",
    "    counts = np.bincount(cluster_prediction)\n",
    "    dominant_cluster = np.argmax(counts)\n",
    "    \n",
    "    #convert to dataframe for ease of use\n",
    "    df = pd.DataFrame(embbedings)\n",
    "    \n",
    "    #include the cluster labels\n",
    "    df[\"clusters\"] = cluster_prediction\n",
    "    \n",
    "    #choosing the dominant cluster ebbedings & calculate the mean of them to produce the attribute\n",
    "    dominant_embbedings = df[df[\"clusters\"] == dominant_cluster]\n",
    "    dominant_embbedings = dominant_embbedings.drop([\"clusters\"], axis=1)\n",
    "    dominant_embbedings_nparray = dominant_embbedings.to_numpy()\n",
    "    the_attribute = np.mean(dominant_embbedings_nparray, axis=0)\n",
    "    \n",
    "    return the_attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29f7cd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalizeData(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "661c39a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myatt= att_generator(my_embbedings)\n",
    "myatt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed4b0a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 768)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myatt = myatt.reshape(1, 768)\n",
    "attribute_array = np.zeros([1, 768])\n",
    "asghar = np.append(attribute_array, myatt, axis=0)\n",
    "\n",
    "asghar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e6f9bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classes = data[\"class\"].unique()\n",
    "attribute_array = np.zeros([1, 768])\n",
    "\n",
    "for i in all_classes:\n",
    "    \n",
    "    my_embbedings, my_labels = embbedings_labels_generator(data, [i], intermediate_layer_model)\n",
    "    myatt = att_generator(my_embbedings)\n",
    "    myatt = myatt.reshape(1, 768)\n",
    "    \n",
    "    attribute_array = np.append(attribute_array, myatt, axis=0)\n",
    "\n",
    "attribute_array = np.delete(attribute_array, 0, 0)\n",
    "attribute_array = attribute_array.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6adef1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 50)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribute_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30a44320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 50)\n"
     ]
    }
   ],
   "source": [
    "the_mat_file = scipy.io.loadmat('att_splits_AWA2.mat')\n",
    "the_mat_file[\"att\"] = attribute_array\n",
    "\n",
    "print(the_mat_file[\"att\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b62a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.io.savemat('myatt_splits768_0.05Val.mat', the_mat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b4b8aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ca8d57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_mat_file = scipy.io.loadmat('myatt_splits768_0.05Val.mat')\n",
    "the_mat_file[\"att\"] = NormalizeData(the_mat_file[\"att\"])\n",
    "\n",
    "scipy.io.savemat('myatt_splits768norm_0.05Val.mat', the_mat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca84a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "piptext",
   "language": "python",
   "name": "piptext"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
